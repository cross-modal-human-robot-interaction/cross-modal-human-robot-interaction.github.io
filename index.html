<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>Human Interaction for Robot Navigation - Index</title>
  <meta content="" name="description">
  <meta content="" name="keywords">

  <!-- Favicons -->
  <link href="assets/img/favicon.png" rel="icon">
  <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/icofont/icofont.min.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/venobox/venobox.css" rel="stylesheet">
  <link href="assets/vendor/line-awesome/css/line-awesome.min.css" rel="stylesheet">
  <link href="assets/vendor/owl.carousel/assets/owl.carousel.min.css" rel="stylesheet">
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">

  <!-- =======================================================
  * Template Name: Serenity - v2.2.1
  * Template URL: https://bootstrapmade.com/serenity-bootstrap-corporate-template/
  * Author: BootstrapMade.com
  * License: https://bootstrapmade.com/license/
  ======================================================== -->
</head>

<body>

  <!-- ======= Header ======= -->
  <header id="header" class="fixed-top">
    <div class="container d-flex">

      <div class="logo mr-auto">
        <!-- <h1 class="text-light"><a href="index.html">Cross-modal Human-robot Interaction (2nd)</a></h1> -->
        <!-- Uncomment below if you prefer to use an image logo -->
        <a href="index.html"><img src="assets/img/ECCV-logo3.png" alt="" class="img-fluid"></a>
      </div>

      <nav class="nav-menu d-none d-lg-block">
        <ul>
          <li><a href="call.html">Call for submissions</a></li>
          <li><a href="speakers.html">Speakers</a></li>
          <li><a href="organizers.html">Organizers</a></li>
          <li><a href="schedule.html">Schedule</a></li>
          <li><a href="challenge.html">Challenge</a></li>
        </ul>
      </nav><!-- .nav-menu -->

    </div>
  </header><!-- End Header -->

  <!-- ======= Hero Section ======= -->
  <section id="hero">
    <div class="hero-container" data-aos="fade-up">
      <h1>Cross-modal Human-robot Interaction (2nd)</h1>
      <br> 
      <br> 
      <h2><b>When:</b> Oct, 24, 2022
        <b>Where:</b> Virtual (ECCV 2022 Workshop)</h2>
      <!-- <a href="#about" class="btn-get-started scrollto">Get Started</a> -->
    </div>
  </section><!-- End Hero -->

  <main id="main">

    <section id="about" class="about">

    <div class="container" data-aos="fade-up">

      <div class="row">

        <div class="col-lg-12" style="height: 30px"></div>

        <div class="col-lg-2"></div>

        <div class="col-lg-8">
          <h2> Workshop Information </h2>
          <!-- <strong>The video of our workshop has been avaliable <a href="https://www.youtube.com/watch?v=a-PgM2P5GSQ">Here</a>. </strong>  -->
          <strong>The 1st workshop Human Interaction for Robotic Navigation is avaliable <a href="https://human-interaction4robotic-navigation.github.io/index.html">Here</a>. </strong> 
        </div>

        <div class="col-lg-2"></div>

        <div class="col-lg-12" style="height: 30px"></div>

        <div class="col-lg-2"></div>

        <div class="col-lg-8">
          A long-term goal of AI research is to build intelligent agents that can see the rich visual environment around us, 
          interact with humans in multiple modalities, and act in a physical or embodied environment. 
          As one of the most promising directions, cross-modal human-robot interaction has increasingly attracted attention 
          from both academic and industry fields. The community has developed numerous methods to address the problems in 
          cross-modal human-robot interaction. Visual recognition methods like detection and segmentation enable the robot 
          to understand the semantics in an environment. Large-scale pretraining methods and cross-modal representation 
          learning aim at effective cross-modal alignment. Reinforcement learning methods are applied to learn human-robotic 
          interaction policy. Moreover, the community requires the agent to have other abilities such as life-long/incremental 
          learning or active learning, which broadens the application of real-world human-robot interaction. 
        </div>
        
        <div class="col-lg-2"></div>

        <div class="col-lg-12" style="height: 30px"></div>

        <div class="col-lg-2"></div>
        
        <div class="col-lg-8">
          Many research works have been devoted to related topics, leading to rapid growth of related publications in the top-tier 
          conferences and journals such as CVPR, ICCV, ECCV, NeurIPS, ACL, EMNLP, T-PAMI, etc. We believe this workshop will 
          be a very successful one and it will indeed benefit the progress of human-robot interaction significantly. 
        </div>

        <div class="col-lg-2"></div>

        <div class="col-lg-12" style="height: 30px"></div>

        <div class="col-lg-2"></div>

        <div class="col-lg-8">
          Our workshop are expected to give a promising direction of cross-modal human-robot interaction, 
          which will cover but not limit to the following topics:
          <ul>
            <li>Large-scale cross-modal pretraining, cross-modal representation learning, cross-modal reasoning. </li>
            <li>Vision-language grounding, visual question answering, visual dialogue, visual commonsense reasoning, 
              vision-language navigation, vision-dialog navigation. </li>
            <li>Reinforcement learning, policy exploration for making decisions about cross-modal interaction. </li>
            <li>Self-supervised learning, life-long/incremental learning, active learning. </li>
            <li>Real world cross-modal interaction applications involving humans, e.g. smart assistant, indoor robots, auto-driving, medical diagnosis etc.</li>
            <li>New benchmarks that evaluate the benefit of multi-modal reasoning and interaction approaches in specific scenarios. </li>
          </ul>
        </div>
      </div>
    </div>
  </section>
  </main><!-- End #main -->

  <!-- ======= Footer ======= -->
  <footer id="footer">
    <div class="container">
      <div class="credits">
        Contact Us: <a href="mailto: hi4rn@googlegroups.com">hi4rn@googlegroups.com</a>
      </div>
    </div>
  </footer><!-- End Footer -->

  <a href="#" class="back-to-top"><i class="icofont-simple-up"></i></a>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/jquery/jquery.min.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/jquery.easing/jquery.easing.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>
  <script src="assets/vendor/waypoints/jquery.waypoints.min.js"></script>
  <script src="assets/vendor/counterup/counterup.min.js"></script>
  <script src="assets/vendor/venobox/venobox.min.js"></script>
  <script src="assets/vendor/owl.carousel/owl.carousel.min.js"></script>
  <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="assets/vendor/aos/aos.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>

</body>

</html>
